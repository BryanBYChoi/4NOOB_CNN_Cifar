{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_rev2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0OuXurwLhGkzA7PqJrMpT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxIyYPGWuza4",
        "outputId": "9b930d8a-804d-4b64-8532-24d70a2237ef"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'1단계_코드구현 및작동'\n",
            " all_examples_train.txt\n",
            " all_index_sentences_test.txt\n",
            " all_index_sentences_train.txt\n",
            " all_index_senterces_validation.txt\n",
            " all_word_pos_sentences_all.txt\n",
            " all_word_pos_sentences_test.txt\n",
            " all_word_pos_sentences_train.txt\n",
            " all_word_pos_sentences_validation.txt\n",
            " all_word_pos_sentenes_train.txt\n",
            " autoencoder_denoising.h5\n",
            "'Benchmark_FixRes.ipynb의 사본'\n",
            "'CNN2018 - 5 - [Class Exercise] CNN Case Studies.ipynb의 사본'\n",
            " Denoising_001.ipynb\n",
            "'Denoising_001.ipynb의 사본'\n",
            "'Denoising_001.ipynb의 사본 (1)'\n",
            "'Denoising_001.ipynb의 사본 (2)'\n",
            "'Denoising_001.ipynb의 사본 (3)'\n",
            "'Denoising_001.ipynb의 사본 (4)'\n",
            "'Denoising_001.ipynb의 사본 (5)'\n",
            " Denoising_V3.07_제출용.ipynb\n",
            " file_names_all.txt\n",
            " file_names_test.txt\n",
            " file_names_train.txt\n",
            " file_names_validation.txt\n",
            " fix2.ipynb\n",
            " FixRes_001.ipynb\n",
            "'FixRes_001.ipynb의 사본'\n",
            " kagglecatsanddogs_3367a.zip\n",
            " kimsunju.ipynb\n",
            "'MSR-LA - 3467.docx'\n",
            "'MURA 001.ipynb'\n",
            " MURA_densenet_001\n",
            "'MURA_densenet_001의 사본'\n",
            "'MURA_densenet_001의 사본 (1)'\n",
            " mura_fixRes\n",
            "'MURA SIMPLE2.ipynb'\n",
            "'MURA SIMPLE.ipynb'\n",
            " NN_rev1\n",
            "'NN_rev1의 사본'\n",
            "'NN_rev1의 사본 (1)'\n",
            "'NN_rev1의 사본 (2)'\n",
            " PetImages\n",
            " PetImages_Sub\n",
            "'readme[1].txt'\n",
            " Untitled\n",
            " Untitled0.ipynb\n",
            " VGG-002\n",
            " VGG-003\n",
            " VGG-004\n",
            " VGG-005_renet\n",
            " VGG16-001\n",
            " VGG16-001.ipynb\n",
            " WSJ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGfVWWd1thka",
        "outputId": "0f9faef9-08af-4ac7-cae1-6352f967cd1a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJwF-Pf6tyQB",
        "outputId": "b408d58b-388c-48b7-e47b-60ffa41ce603"
      },
      "source": [
        "cd gdrive/My Drive/Colab Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY9nwV_Y0lar"
      },
      "source": [
        "# 불러오는 부분\n",
        " \n",
        "import os \n",
        "import time \n",
        "import tensorflow as tf \n",
        "from tensorflow. keras . layers import Embedding \n",
        "from tensorflow. keras import layers \n",
        "from tensorflow import keras \n",
        "import numpy as np \n",
        "d_embed = 100\n",
        "window_size = 5\n",
        "Threshold_freq =1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3mkeTvr1PaF"
      },
      "source": [
        "fp = open(\"file_names_all.txt\", \"w\", encoding=\"utf-8\")\n",
        "cur_dir_path = './WSJ'\n",
        "dir_list = os.listdir(cur_dir_path)\n",
        "list_all_file_paths = []\n",
        "for a_dir in sorted(dir_list):\n",
        "    path = os.path.join(cur_dir_path, a_dir)\n",
        "    subdir_list = os.listdir(path)\n",
        "    for a_file in sorted(subdir_list):\n",
        "        if not(a_file[-1]=='S' and a_file[-2]=='0'):\n",
        "            continue\n",
        "        path2 = os.path.join(path, a_file)\n",
        "        fp.write(path2+'＼n')\n",
        "        list_all_file_paths.append(path2)\n",
        "fp.close()\n",
        " \n",
        "# divide the whole corpus to 3 parts! training. validation. test\n",
        "fp_tr = open(\"./file_names_train.txt\", \"w\", encoding=\"utf-8\")\n",
        "fp_va = open(\"./file_names_validation.txt\", \"w\", encoding=\"utf-8\")\n",
        "fp_te = open(\"./file_names_test.txt\", \"w\", encoding=\"utf-8\")\n",
        " \n",
        "total_num_files = len(list_all_file_paths)\n",
        "num_files_train = int(total_num_files * 0.8)\n",
        "num_files_validation = int(total_num_files * 0.1)\n",
        "num_files_test = total_num_files - (num_files_train + num_files_validation)\n",
        " \n",
        "for i in range(num_files_train):\n",
        "    fp_tr.write(list_all_file_paths[i] + '\\n')\n",
        "fp_tr.close()\n",
        " \n",
        "for i in range(num_files_validation):\n",
        "    fp_va.write(list_all_file_paths[num_files_train + i] + '\\n')\n",
        "fp_va.close()\n",
        " \n",
        "for i in range(num_files_test):\n",
        "    fp_te.write(list_all_file_paths[num_files_train + num_files_validation + i] + '\\n')\n",
        "fp_te.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k06KaXdPExt"
      },
      "source": [
        "def build_word_pos_sentences(path_file_names, path_result_file):\n",
        "    fp_n = open(path_file_names, \"r\", encoding=\"utf-8\")\n",
        "    fp_s = open(path_result_file, \"w\", encoding=\"utf-8\")\n",
        " \n",
        "    while True:\n",
        "        filename = fp_n.readline()\n",
        " \n",
        "        if len(filename) == 0:\n",
        "            break     # enf of file reached.\n",
        " \n",
        "        filename = filename[:-1]\n",
        "        sentences = read_a_file_and_form_sentences(filename)\n",
        "        fp_s.write('<<' + filename + '>> Num sentences:' + str(len(sentences)) + '\\n')\n",
        " \n",
        "        # store word/pos in file all word pos seaence ????.txt'.\n",
        "        for sentence in sentences:\n",
        "            s_line = '' # initialize with emotv string.\n",
        "            for w_p in sentence:\n",
        "                s_line = s_line + '' + w_p\n",
        "            fp_s.write(s_line + '\\n')\n",
        "    fp_n.close()\n",
        "    fp_s.close()\n",
        " \n",
        "build_word_pos_sentences(\"./file_names_all.txt\", \"./all_word_pos_sentences_all.txt\")\n",
        " \n",
        " \n",
        "build_word_pos_sentences(\"./file_names_train.txt\", \"./all_word_pos_sentences_train.txt\")\n",
        "build_word_pos_sentences(\"./file_names_validation.txt\", \"./all_word_pos_sentences_validation.txt\")\n",
        "build_word_pos_sentences(\"./file_names_test.txt\", \"./all_word_pos_sentences_test.txt\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CywN3D0VPFHI"
      },
      "source": [
        "def read_a_file_and_from_sentences (file_path):\n",
        "    fp = open(file_path, \"r\", encoding=\"utf-8\")\n",
        " \n",
        "    sentences_of_file = [] # this will have lists. where each list will have word/pos Dairs of a sentence.\n",
        " \n",
        "    while True: # this is for a sentence\n",
        "        sentence = []\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            leng = len(line)\n",
        "            if leng == 0:\n",
        "                # end of file has come. So finish this file. Terminate this function with returing the result\n",
        "                # note that there is nothing to insert sentence into sentences of file.\n",
        "                return sentences_of_file\n",
        "            elif leng == 1 or (line[0] == '=' and line[1] == '=' and line[2] == '='):\n",
        "                continue\n",
        "            else:\n",
        "                # first line of a sentence has come. Store its elements.\n",
        "                line = line[:-1]\n",
        "                line_splited = line.split()\n",
        "                for ele in line_splited:\n",
        "                    if ele != '[' and ele != ']':\n",
        "                        sentence.append(ele)\n",
        "                break\n",
        " \n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            leng = len(line)\n",
        "            if leng == 0: # end of file has come. so finish this file.\n",
        "                break\n",
        "            elif leng == 1 or (leng >= 4 and (liner[0] == '=' and line[1] == '=' and liner[2] == '=')):\n",
        "                # the first garbage line after the last line of a sentence has come.\n",
        "                break\n",
        "            else:\n",
        "                # next line of a sentence has come.\n",
        "                line = line[:-1]\n",
        "                line_splited = line.split()\n",
        " \n",
        "                for ele in line_splited:\n",
        "                    if not(ele == '[' or ele == ']'):\n",
        "                        sentence.append(ele)\n",
        " \n",
        "        sentences_of_file.append(sentence)\n",
        " \n",
        "        if leng == 0:    # the end of file has come. So we need to terminate the function.\n",
        "            break\n",
        "    return sentences_of_file"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KjC8QSDPFPZ"
      },
      "source": [
        "def build_vocabulary_temp(corpus_path):\n",
        "    fp = open(corpus_path, \"r\", encoding=\"utf-8\")\n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':\n",
        "            continue\n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            word = w_p[0]\n",
        "            pos = w_p[1]\n",
        "            if word in Vocab_temporary:\n",
        "                Vocab_temporay[word] += 1\n",
        "            else:\n",
        "                Vocab_temporay[word] = 1\n",
        "    fp.close()\n",
        "    return\n",
        " \n",
        "# Create Vocab(a Vocabulary) of our system by taking words (in train part of the tagged corpus)\n",
        "# of fred greater or equal to a given threshold.\n",
        " \n",
        "Vocab_temporary = {}\n",
        "build_vocabulary_temp(\"./all_word_pos_sentences_all.txt\")\n",
        "sorted_Vocab = sorted(Vocab_temporary.items(), key = lambda kv: kv[1], reverse=True) # this\n",
        "Total_n_words = len(sorted_Vocab)\n",
        " \n",
        "Vocab = {} # This will be our final vocab.\n",
        "n_reserved = 2 # currently. 「PAD1 of index 0. [UNK] of index 1.\n",
        "for i in range(Total_n_words):\n",
        "    word = sorted_Vocab[i][0]\n",
        "    freq = sorted_Vocab[i][1]\n",
        " \n",
        "    if fred >= Threshold_freq:\n",
        "        Vocab[word] = i + n_reserved # give index to word.\n",
        "    else:\n",
        "        # all the cemaining will be ones of fred less than threshold frea.\n",
        "        break\n",
        " \n",
        "Vocab['[PAD]'] = 0 # index of pad word\n",
        "Vocab['[UNK]'] = 1 # index of unknown word\n",
        "Vocab_size = len(Vocab) # include 「PAD1. 「UNK1\n",
        " \n",
        "# Create our dictionary for part of speech, dic POS, from original paper of Penn-tree bank\n",
        "dic_POS = {}\n",
        "all_pos_list = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS', \\\n",
        "                'MD','NN','NNS','NNP','NNPS','PDT','POS', 'PRP', 'PRP$','RB', \\\n",
        "                'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', \\\n",
        "                'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '#', '$', '.', ',', \\\n",
        "                ':', '(', ')']\n",
        "# give a unique index to each POS.\n",
        "for i in range(43):\n",
        "    dic_POS[all_pos_list[i]] = i"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIjrYY0hPFWs"
      },
      "source": [
        "def build_index_sentences(path_word_pos_sentence_file, path_index_sentence_file):\n",
        "    fp = open(path_word_pos_sentence_file, \"r\", encoding=\"utf-8\")\n",
        "    fp_w = open(path_index_sentence_file, \"w\", encoding=\"utf-8\")\n",
        " \n",
        "    for line in fp.readlines():\n",
        "        sentence = line.split()\n",
        "        if sentence[0] == '<<':\n",
        "            continue\n",
        " \n",
        "        line_widx = '' # line for word indices\n",
        "        line_pidx = '' # line for DOS indices\n",
        " \n",
        "        for word_pos_pair in sentence:\n",
        "            w_p = word_pos_pair.split('/')\n",
        "            nseg = len(w_p)\n",
        "            if nseg > 2:\n",
        "                word = ''\n",
        "                for i in range(nseg-1):\n",
        "                    word = word + w_p[i] + '/'\n",
        "                word = word[:-1] # remove the last slash.\n",
        "            else:\n",
        "                word = w_p[0]\n",
        " \n",
        "            Pos = w_p[-1] # the last segment\n",
        "            if not(word in Vocab):\n",
        "                widx = 1 # give index of 「UNK1 since it is missing in Vocab.\n",
        "            else:\n",
        "                widx = Vocab[word]\n",
        " \n",
        "            if not(pos in dic_POS):\n",
        "                pos_list = pos.split('|')\n",
        "                pos = pos_list[-1]\n",
        "                if not (pos in dic_POS):\n",
        "                    print(\"exception occurs at dic POS look up. W D=\", w_p, \" pos=\", pos)\n",
        "                    time.sleep(100)\n",
        "                else:\n",
        "                    pidx = dic_POS[pos]\n",
        "            else:\n",
        "                pidx = dic_POS[pos]\n",
        " \n",
        "            if len(line_widx) == 0:\n",
        "                line_widx = line_widx + str(widx)\n",
        "            else:\n",
        "                line_widx = line_widx + '\\t' + str(widx)\n",
        " \n",
        "            if len(line_pidx) == 0:\n",
        "                line_pidx = line_pidx + str(pidx)\n",
        "            else:\n",
        "                line_pidx = line_pidx + '\\t' + str(pidx)\n",
        " \n",
        "        fp_w.write(line_widx + '\\n')\n",
        "        fp_w.write(line_pidx + '\\n')\n",
        "        fp_w.write:('\\n') # an empty line after each sentence\n",
        "    fp_w.close()\n",
        "    fp.close()\n",
        " \n",
        "build_index_sentences(\"./all_word_pos_sentences_train.txt\", \"./all_index_sentences_train.txt\")\n",
        "build_index_sentences(\"./all_word_pos_sentences_validation.txt\", \"./all_index_sentences_validation.txt\")\n",
        "build_index_sentences(\"./all_word_pos_sentences_test.txt\", \"./all_index_sentences_test.txt\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb3Hpy3QPFiT"
      },
      "source": [
        "def build_examples(path_index_file, path_examples_file, window_size):\n",
        "    fp = open(path_index_file, \"r\", encoding=\"utf-8\")\n",
        "    fp_w = open(path_examples_file, \"w\", encoding=\"utf-8\")\n",
        " \n",
        "    fp_w.write(str(window_size)+\" word indices and a pos label.\\n\")\n",
        "          # insert first line as comment.\n",
        "    num_pads = window_size // 2\n",
        "    while True:\n",
        "        # read two lines\n",
        "        wline = fp.readline()\n",
        "        if len(wline) == 0:\n",
        "            break # end of file has come.\n",
        "        pline = fp.readline()\n",
        "        w_index = wline.split()\n",
        "        p_index = pline.solit()\n",
        " \n",
        "        # pad (index 0) are placed before the sentence\n",
        "        pad_added_word_index = []\n",
        "        for i in range(num_pads):\n",
        "            pad_added_word_index.append('0')\n",
        "        \n",
        "        for widx in w_index:\n",
        "            pad_added_word_index.append(widx)\n",
        "        \n",
        "        # pade are added after sentence\n",
        "        for i in range(num_pads):\n",
        "            pad_added_word_index.append('0')\n",
        "        \n",
        "        leng_sentence = len(w_index)\n",
        "        for i in range(leng_sentence):\n",
        "            outline = ''\n",
        "            for j in range(window_size):\n",
        "                outline += pad_added_word_index[i+j]\n",
        "                outline += '\\t'\n",
        "            outline += p_index[i] + '\\n'\n",
        "            fp_w.write(outline)\n",
        " \n",
        "        line = fp.readline()\n",
        "        if len(line) != 1:\n",
        "            print(\"logic error. length 1 line expected in file all index sentences.txt\")\n",
        "            time.sleep(100)\n",
        "        \n",
        "    fp.close()\n",
        "    fp_w.close()\n",
        " \n",
        "build_examples(\"./all_index_sentences_train.txt\", \"./all_examples_train.txt\", window_size)\n",
        "build_examples(\"./all_index_sentences_validation.txt\", \"./all_examples_validation.txt\", window_size)\n",
        "build_examples(\"./all_index_sentences_test.txt\", \"./all_examples_test.txt\", window_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVDNFrp8PFxy"
      },
      "source": [
        "# Model design\n",
        "embedding_layer = Embedding(Vocab_size, d_embed, embeddings_initializer='random_normal', \n",
        "                            input_length=window_size, trainable=True)\n",
        "int_seq = keras.Input(shape=(window_size), dtype=\"int64\")\n",
        "emb_seq = embedding_layer(int_seq)\n",
        "x = layers.Flatten()(emb_seq)\n",
        "x = layers.Dense(640, activation=\"relu\")(x)\n",
        "x = layers.Dense(640, activation=\"relu\")(x)\n",
        "preds = layers.Dense(48, activation=\"softmax\")(x)\n",
        " \n",
        "model = keras.Model(int_seq, preds)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qu2jj1rPFqq"
      },
      "source": [
        "# Read training examples for train and validation from example files,\n",
        "# Lists xtrain. y train: x and d of training examples from training data.\n",
        "# Lists X validation. v validation: the same as the above.\n",
        " \n",
        " \n",
        "def read_examples_from_file(path_examples_file):\n",
        "    fp = open(path_examples_file, \"r\", encoding=\"utf-8\")\n",
        "    \n",
        "    X = []\n",
        "    Y = []\n",
        "    for i, line in enumerate(fp.readlines()): \n",
        "        if i == 0:\n",
        "            continue # ignore the first line which is a comment\n",
        "        line_split = line.split()\n",
        "        intseq = [int(a) for a in line_split]\n",
        "        an_inputX = intseq[0:wirldow_size]\n",
        "        X.append(an_inputX) # X (an input word index sea)\n",
        "        Y.append(intseq[-1]) # d (a label not a 1-hot vector)\n",
        "    return X, Y\n",
        " \n",
        "x_train, y_train = read_examples_from_file(\"./all_examples_train.txt\")\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        " \n",
        " \n",
        "x_validation, y_validation = read_examples_from_file(\"./all_examples_validation.txt\")\n",
        "x_validation = np.array(x_validation)\n",
        "y_validation = np.array(y_validation)\n",
        " \n",
        "x_test, y_test = read_examples_from_file(\"./all_examples_test.txt\")\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "k_tDWrYwPF7g",
        "outputId": "09b77f3e-47bc-4c8e-b54e-bc46bfcd4d97"
      },
      "source": [
        "# loss sparse categorical crossentropy: target is given as a label index than one-hot vector.\n",
        "LEARNING_RATE = 0.3e-3\n",
        "DECAY = 0.00005\n",
        "EPOCHS = 5\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, decay=DECAY)\n",
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n",
        " \n",
        "model.fit(x_train, y_train, batch_size=64, epochs=EPOCHS, validation_data=(x_validation,\n",
        "                        y_validation), shuffle=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-22bdf886f9b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m model.fit(x_train, y_train, batch_size=64, epochs=EPOCHS, validation_data=(x_validation,\n\u001b[0;32m----> 9\u001b[0;31m                         y_validation), shuffle=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3mAPZbMsKHq"
      },
      "source": [
        "## TEST ##\n",
        "pred = model.predict(x=x_test, verbose=1) # batch 단위로 수행함 (default - 32)\n",
        "                                          # batch 별 결과를 므아서 전체 결과를 반환함\n",
        "pred_label = tf.math.argmax(pred, axis=1)\n",
        "leng_test = len(ytest)\n",
        "equality = tf.math.equal(pred_label, y_test)  # this is a boolean array.\n",
        "compare = np.ones(leng_test, dtype=np.int64)\n",
        "for i in range(leng_test):\n",
        "    if not equality[i]:\n",
        "        compare[1] = 0\n",
        "hit = tf.reduce_sum(compare)\n",
        "acc = hit / leng test\n",
        "print(\"test accuracy =\", acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}